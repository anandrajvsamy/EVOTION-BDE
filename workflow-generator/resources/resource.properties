# Note :- All values are string values , do not use "" for specify any parameter  
# 
# job_tracker is the job tracker url please don't put http:// before hostname valid input "localhost:54311"
# you will get job_tracker value in mapred-site.xml
job_tracker = sandbox-hdp.hortonworks.com:8032
# name node please look into hdfs-site.xml for getting namenode url , use hdfs://{hostname}:{port}
name_node = hdfs://sandbox-hdp.hortonworks.com:8020

#node name where spark is installed
spark_master = yarn-cluster

#folder path where your jars stored
jars_path = hdfs://sandbox-hdp.hortonworks.com:8020/user/root/

#extra spark options
SparkOpts = --jars hdfs://sandbox-hdp.hortonworks.com:8020/user/root/phoenix-4.7.0.2.6.5.0-292-client.jar
#hbase jdbc url
hbase_url = jdbc:phoenix:172.18.0.2:/hbase-unsecure

#input path workflow input
input = /input/hdfs/path

#output path for workflow output
output = output/hdfs/path

#oozie client where we need to execute job
oozie_client = http://localhost:11000/oozie/


#hdfs output path of xml file
output_workflow_path = /workflow2.xml

#local file location of server 
input_local_path = /Users/Anandraj/Documents/workflow_output/workflow.xml

#path for oozie library 
oozie_lib_path = /oozie/lib

#local path for core and hdfs site xml 
hadoop_core_xml = /etc/hadoop/conf/core-site.xml
hadoop_hdfs_xml = /etc/hadoop/conf/hdfs-site.xml

input_file  = Independent_Tasks.json





#oozie_namespace = uri:oozie:spark-action:0.4